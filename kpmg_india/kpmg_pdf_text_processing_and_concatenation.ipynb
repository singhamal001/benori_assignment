{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parquet saved as final_concatenated_insights_gzip.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert Treebank tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize textual data by lowercasing, removing punctuation, and extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    \"\"\"\n",
    "    Remove stop words from the list of tokens.\n",
    "    \"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize the tokens.\n",
    "    \"\"\"\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def is_similar(title1, title2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compares two titles using token-level matching.\n",
    "    The similarity is computed as:\n",
    "        similarity = (number of common tokens) / (max(tokens in title1, tokens in title2))\n",
    "    Returns True if similarity is greater than or equal to the threshold.\n",
    "    \"\"\"\n",
    "    tokens1 = set(lemmatize(word_tokenize(title1)))\n",
    "    tokens2 = set(lemmatize(word_tokenize(title2)))\n",
    "    if not tokens1 or not tokens2:\n",
    "        return False\n",
    "    common = tokens1.intersection(tokens2)\n",
    "    similarity = len(common) / max(len(tokens1), len(tokens2))\n",
    "    return similarity >= threshold\n",
    "\n",
    "def extract_pdf_title(filename):\n",
    "    \"\"\"\n",
    "    Extracts the title part from a text filename formatted as \"KPMG_DATE_TITLE.txt\".\n",
    "    It splits the filename (without the extension) by '_' with a maximum of 2 splits.\n",
    "    For example:\n",
    "      \"KPMG_2025-02-06_KPMG global tech report 2024.txt\" -> \"KPMG global tech report 2024\"\n",
    "    \"\"\"\n",
    "    base = filename[:-4]\n",
    "    parts = base.split('_', 2)\n",
    "    if len(parts) == 3:\n",
    "        return parts[2].strip()\n",
    "    else:\n",
    "        return base\n",
    "\n",
    "# --------------------------\n",
    "# Step 1. Load the Original CSV\n",
    "# --------------------------\n",
    "csv_file = \"insights-details-kpmg.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# --------------------------\n",
    "# Step 2. Build a List of PDF Text Files Info from the \"txt\" Folder\n",
    "# --------------------------\n",
    "txt_folder = \"txt\"\n",
    "pdf_text_files = []\n",
    "for file in os.listdir(txt_folder):\n",
    "    if file.lower().endswith('.txt'):\n",
    "        pdf_title = extract_pdf_title(file)\n",
    "        file_path = os.path.join(txt_folder, file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        pdf_text_files.append({\n",
    "            \"filename\": file,\n",
    "            \"pdf_title\": pdf_title,\n",
    "            \"pdf_content\": content\n",
    "        })\n",
    "\n",
    "# --------------------------\n",
    "# Step 3. Process Each CSV Row, Find Matching PDF Content, and Concatenate\n",
    "# --------------------------\n",
    "new_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    csv_title = str(row.get(\"Title\", \"\")).strip()\n",
    "\n",
    "    matched_pdf_content = \"\"\n",
    "    for pdf_file in pdf_text_files:\n",
    "        pdf_title = pdf_file[\"pdf_title\"]\n",
    "        if is_similar(csv_title, pdf_title, threshold=0.5):\n",
    "            matched_pdf_content = pdf_file[\"pdf_content\"]\n",
    "            break\n",
    "\n",
    "    csv_content = str(row.get(\"Content\", \"\"))\n",
    "    concatenated = csv_content + \" \" + matched_pdf_content\n",
    "\n",
    "    # Normalize concatenated text\n",
    "    normalized_text = normalize_text(concatenated)\n",
    "    \n",
    "    # For the \"tokenized_text\" column: generate only 2-gram tokens\n",
    "    words = word_tokenize(normalized_text)\n",
    "    two_gram_tokens = ['_'.join(ngram) for ngram in ngrams(words, 2)]\n",
    "    tokenized_text = ' '.join(two_gram_tokens)\n",
    "    \n",
    "    # For the \"normalized_concatenated_text\" column: only word-level tokens\n",
    "    word_tokens = word_tokenize(normalized_text)\n",
    "    filtered_tokens = remove_stop_words(word_tokens)\n",
    "    lemmatized_tokens = lemmatize(filtered_tokens)\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    new_row = {\n",
    "        \"url_link\": row.get(\"url_link\", row.get(\"Link\", \"\")),\n",
    "        \"Title\": csv_title,\n",
    "        \"Description\": row.get(\"Description\", \"\"),\n",
    "        \"Date\": row.get(\"Date\", \"\"),\n",
    "        \"Content\": csv_content,\n",
    "        \"Pdf_link\": row.get(\"Pdf_link\", \"\"),\n",
    "        \"pdf_content\": matched_pdf_content,\n",
    "        \"concatenated_text\": concatenated,  # Original concatenated text\n",
    "        \"tokenized_text\": tokenized_text,    # Contains only 2-gram tokens\n",
    "        \"normalized_concatenated_text\": lemmatized_text  # Contains normalized, stop-word removed, and lemmatized words\n",
    "    }\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4. Save the New Data to a New CSV File\n",
    "# --------------------------\n",
    "new_df = pd.DataFrame(new_data)\n",
    "output_parquet = \"final_concatenated_insights_gzip.parquet\"\n",
    "\n",
    "new_df.to_parquet(output_parquet, compression='gzip')\n",
    "\n",
    "print(f\"Final parquet saved as {output_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'normalized_concatenated_text': 41062.71428571428\n",
      "Average length of 'tokenized_text': 108290.0\n",
      "Average length of 'concatenated_text': 56169.57142857143\n"
     ]
    }
   ],
   "source": [
    "lengths = new_df['normalized_concatenated_text'].apply(len)\n",
    "average_length = lengths.mean()\n",
    "\n",
    "print(\"Average length of 'normalized_concatenated_text':\", average_length)\n",
    "\n",
    "\n",
    "lengths = new_df['tokenized_text'].apply(len)\n",
    "average_length = lengths.mean()\n",
    "\n",
    "print(\"Average length of 'tokenized_text':\", average_length)\n",
    "\n",
    "lengths = new_df['concatenated_text'].apply(len)\n",
    "average_length = lengths.mean()\n",
    "\n",
    "print(\"Average length of 'concatenated_text':\", average_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>Pdf_link</th>\n",
       "      <th>pdf_content</th>\n",
       "      <th>concatenated_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>normalized_concatenated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://kpmg.com/in/en/insights/2025/02/aau-ac...</td>\n",
       "      <td>Issue no. 103 | February 2025</td>\n",
       "      <td>This edition of AAU covers relevant financial ...</td>\n",
       "      <td>2025-02-28 00:00:00</td>\n",
       "      <td>Ind AS 103, Business Combination provides guid...</td>\n",
       "      <td>https://kpmg.com/content/dam/kpmgsites/in/pdf/...</td>\n",
       "      <td>February 2025\\nkpmg.com/in\\nAccounting and \\nA...</td>\n",
       "      <td>Ind AS 103, Business Combination provides guid...</td>\n",
       "      <td>ind as 103 business combination provides guida...</td>\n",
       "      <td>ind 103 business combination provide guidance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://kpmg.com/in/en/insights/2025/02/food-a...</td>\n",
       "      <td>Food and Nutritional Security in India</td>\n",
       "      <td>Solutions for achieving zero hunger and ensuri...</td>\n",
       "      <td>2025-02-20 00:00:00</td>\n",
       "      <td>Food security has been a critical aspect of In...</td>\n",
       "      <td>https://kpmg.com/content/dam/kpmgsites/in/pdf/...</td>\n",
       "      <td>Food and nutritional \\nsecurity in India\\nSolu...</td>\n",
       "      <td>Food security has been a critical aspect of In...</td>\n",
       "      <td>food security has been a critical aspect of in...</td>\n",
       "      <td>food security critical aspect indias public po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://kpmg.com/in/en/insights/2025/02/financ...</td>\n",
       "      <td>Financial Crime Bulletin</td>\n",
       "      <td>Dive deep into the financial crime avenues and...</td>\n",
       "      <td>2025-02-10 00:00:00</td>\n",
       "      <td>Financial crimes have become an ever-evolving ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Financial crimes have become an ever-evolving ...</td>\n",
       "      <td>financial crimes have become an everevolving p...</td>\n",
       "      <td>financial crime become everevolving problem me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://kpmg.com/in/en/insights/2025/02/kpmg-g...</td>\n",
       "      <td>KPMG global tech report – industrial manufactu...</td>\n",
       "      <td>Interoperability, hybrid models and AI innovat...</td>\n",
       "      <td>2025-02-07 00:00:00</td>\n",
       "      <td>In the rapidly evolving landscape of industria...</td>\n",
       "      <td>https://kpmg.com/content/dam/kpmgsites/xx/pdf/...</td>\n",
       "      <td>KPMG global \\ntech report 2024\\nKPMG Internati...</td>\n",
       "      <td>In the rapidly evolving landscape of industria...</td>\n",
       "      <td>in the rapidly evolving landscape of industria...</td>\n",
       "      <td>rapidly evolve landscape industrial manufactur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://kpmg.com/in/en/insights/2025/02/kpmg-g...</td>\n",
       "      <td>KPMG global tech report: Technology insights</td>\n",
       "      <td>Tech: A bold sector that innovates while leadi...</td>\n",
       "      <td>2025-02-07 00:00:00</td>\n",
       "      <td>The digital transformation journey is an impor...</td>\n",
       "      <td>https://kpmg.com/content/dam/kpmgsites/xx/pdf/...</td>\n",
       "      <td>KPMG global \\ntech report 2024\\nKPMG Internati...</td>\n",
       "      <td>The digital transformation journey is an impor...</td>\n",
       "      <td>the digital transformation journey is an impor...</td>\n",
       "      <td>digital transformation journey important strat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url_link  \\\n",
       "0  https://kpmg.com/in/en/insights/2025/02/aau-ac...   \n",
       "1  https://kpmg.com/in/en/insights/2025/02/food-a...   \n",
       "2  https://kpmg.com/in/en/insights/2025/02/financ...   \n",
       "3  https://kpmg.com/in/en/insights/2025/02/kpmg-g...   \n",
       "4  https://kpmg.com/in/en/insights/2025/02/kpmg-g...   \n",
       "\n",
       "                                               Title  \\\n",
       "0                      Issue no. 103 | February 2025   \n",
       "1             Food and Nutritional Security in India   \n",
       "2                           Financial Crime Bulletin   \n",
       "3  KPMG global tech report – industrial manufactu...   \n",
       "4       KPMG global tech report: Technology insights   \n",
       "\n",
       "                                         Description                 Date  \\\n",
       "0  This edition of AAU covers relevant financial ...  2025-02-28 00:00:00   \n",
       "1  Solutions for achieving zero hunger and ensuri...  2025-02-20 00:00:00   \n",
       "2  Dive deep into the financial crime avenues and...  2025-02-10 00:00:00   \n",
       "3  Interoperability, hybrid models and AI innovat...  2025-02-07 00:00:00   \n",
       "4  Tech: A bold sector that innovates while leadi...  2025-02-07 00:00:00   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Ind AS 103, Business Combination provides guid...   \n",
       "1  Food security has been a critical aspect of In...   \n",
       "2  Financial crimes have become an ever-evolving ...   \n",
       "3  In the rapidly evolving landscape of industria...   \n",
       "4  The digital transformation journey is an impor...   \n",
       "\n",
       "                                            Pdf_link  \\\n",
       "0  https://kpmg.com/content/dam/kpmgsites/in/pdf/...   \n",
       "1  https://kpmg.com/content/dam/kpmgsites/in/pdf/...   \n",
       "2                                                NaN   \n",
       "3  https://kpmg.com/content/dam/kpmgsites/xx/pdf/...   \n",
       "4  https://kpmg.com/content/dam/kpmgsites/xx/pdf/...   \n",
       "\n",
       "                                         pdf_content  \\\n",
       "0  February 2025\\nkpmg.com/in\\nAccounting and \\nA...   \n",
       "1  Food and nutritional \\nsecurity in India\\nSolu...   \n",
       "2                                                      \n",
       "3  KPMG global \\ntech report 2024\\nKPMG Internati...   \n",
       "4  KPMG global \\ntech report 2024\\nKPMG Internati...   \n",
       "\n",
       "                                   concatenated_text  \\\n",
       "0  Ind AS 103, Business Combination provides guid...   \n",
       "1  Food security has been a critical aspect of In...   \n",
       "2  Financial crimes have become an ever-evolving ...   \n",
       "3  In the rapidly evolving landscape of industria...   \n",
       "4  The digital transformation journey is an impor...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ind as 103 business combination provides guida...   \n",
       "1  food security has been a critical aspect of in...   \n",
       "2  financial crimes have become an everevolving p...   \n",
       "3  in the rapidly evolving landscape of industria...   \n",
       "4  the digital transformation journey is an impor...   \n",
       "\n",
       "                        normalized_concatenated_text  \n",
       "0  ind 103 business combination provide guidance ...  \n",
       "1  food security critical aspect indias public po...  \n",
       "2  financial crime become everevolving problem me...  \n",
       "3  rapidly evolve landscape industrial manufactur...  \n",
       "4  digital transformation journey important strat...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
