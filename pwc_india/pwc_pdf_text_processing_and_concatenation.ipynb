{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This script performs the following tasks:\n",
    "1. Extracts text from PDF files.\n",
    "2. Normalizes and processes text data.\n",
    "3. Matches PDF titles with CSV records based on similarity.\n",
    "4. Concatenates matched text and saves it to a compressed Parquet file.\n",
    "\n",
    "## Key Components\n",
    "### Libraries Used\n",
    "- **os**: To handle file and directory operations.\n",
    "- **re**: For regular expression operations.\n",
    "- **pandas**: For data handling.\n",
    "- **nltk**: For natural language processing tasks.\n",
    "\n",
    "### Functions\n",
    "- **get_wordnet_pos()**: Converts NLTK POS tags to WordNet POS tags.\n",
    "- **normalize_text()**: Cleans and prepares text by removing emails, punctuation, and extra spaces.\n",
    "- **remove_stop_words()**: Filters out common English stop words.\n",
    "- **lemmatize_tokens()**: Converts words to their base form using lemmatization.\n",
    "- **extract_pdf_title()**: Extracts titles from PDF filenames.\n",
    "- **is_similar()**: Measures token-level similarity between titles.\n",
    "\n",
    "### Workflow\n",
    "1. **Load CSV Data:** Reads a CSV file containing titles and descriptions.\n",
    "2. **Extract PDF Titles and Content:** Loads text files and extracts titles based on filenames.\n",
    "3. **Match and Concatenate Text:** Matches PDF content to CSV titles using token similarity and concatenates relevant text.\n",
    "4. **Normalize and Tokenize Text:**\n",
    "   - Normalizes text (removes emails, special characters, etc.).\n",
    "   - Tokenizes text into bigrams and removes stop words.\n",
    "   - Lemmatizes tokens.\n",
    "5. **Save to Parquet:** Saves processed data into a compressed Parquet file.\n",
    "\n",
    "## Output\n",
    "- A compressed Parquet file containing matched and processed text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parquet file saved as pwc_final_concatenated_insights_gzip.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert Treebank tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by:\n",
    "      - Removing email addresses (any string containing '@' and ending with .com or .in)\n",
    "      - Lowercasing the text\n",
    "      - Removing punctuation\n",
    "      - Trimming extra whitespace\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\S+@\\S+\\.(com|in)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    \"\"\"Remove stop words from a list of tokens.\"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens using POS tags.\"\"\"\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tokens]\n",
    "\n",
    "def extract_pdf_title(filename):\n",
    "    \"\"\"\n",
    "    Extract the publication title from a filename.\n",
    "    Expected format: \"PWC_DATE_Title.txt\"\n",
    "    Splits the filename by '_' with a maximum of two splits and replaces underscores with spaces.\n",
    "    \"\"\"\n",
    "    base = filename[:-4]\n",
    "    parts = base.split('_', 2)\n",
    "    if len(parts) >= 3:\n",
    "        return parts[2].replace('_', ' ').strip()\n",
    "    else:\n",
    "        return base.replace('_', ' ')\n",
    "\n",
    "def is_similar(title1, title2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compare two titles based on token-level matching.\n",
    "    Returns True if similarity (based on common lemmatized tokens) exceeds the threshold.\n",
    "    \"\"\"\n",
    "    tokens1 = set(lemmatize_tokens(word_tokenize(title1)))\n",
    "    tokens2 = set(lemmatize_tokens(word_tokenize(title2)))\n",
    "    if not tokens1 or not tokens2:\n",
    "        return False\n",
    "    common = tokens1.intersection(tokens2)\n",
    "    similarity = len(common) / max(len(tokens1), len(tokens2))\n",
    "    return similarity >= threshold\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Load the CSV file containing insights details\n",
    "# --------------------------\n",
    "csv_file = \"insights-details-pwc.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Build a list of text file info from the \"txt\" folder\n",
    "# --------------------------\n",
    "txt_folder = \"txt\"\n",
    "pdf_text_files = []\n",
    "for filename in os.listdir(txt_folder):\n",
    "    if filename.lower().endswith('.txt'):\n",
    "        pdf_title = extract_pdf_title(filename)\n",
    "        file_path = os.path.join(txt_folder, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        pdf_text_files.append({\n",
    "            \"filename\": filename,\n",
    "            \"pdf_title\": pdf_title,\n",
    "            \"pdf_content\": content\n",
    "        })\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Process each CSV row to match PDF text and perform concatenation\n",
    "# --------------------------\n",
    "new_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    csv_title = str(row.get(\"Title\", \"\")).strip()\n",
    "    csv_description = str(row.get(\"Description\", \"\")).strip()\n",
    "    matched_pdf_content = \"\"\n",
    "    \n",
    "    for pdf_file in pdf_text_files:\n",
    "        if is_similar(csv_title, pdf_file[\"pdf_title\"], threshold=0.5):\n",
    "            matched_pdf_content = pdf_file[\"pdf_content\"]\n",
    "            break\n",
    "\n",
    "    concatenated = csv_title + \" \" + csv_description + \" \" + matched_pdf_content\n",
    "\n",
    "    normalized = normalize_text(concatenated)\n",
    "    \n",
    "    words = word_tokenize(normalized)\n",
    "    two_grams = ['_'.join(gram) for gram in ngrams(words, 2)]\n",
    "    tokenized_text = ' '.join(two_grams)\n",
    "    \n",
    "    word_tokens = word_tokenize(normalized)\n",
    "    filtered_tokens = remove_stop_words(word_tokens)\n",
    "    lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "    normalized_concatenated_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    new_row = {\n",
    "        \"Date\": row.get(\"Date\", \"\"),\n",
    "        \"Title\": csv_title,\n",
    "        \"Description\": csv_description,\n",
    "        \"Link\": row.get(\"Link\", \"\"),\n",
    "        \"pdf_content\": matched_pdf_content,\n",
    "        \"concatenated_text\": concatenated,\n",
    "        \"tokenized_text\": tokenized_text,\n",
    "        \"normalized_concatenated_text\": normalized_concatenated_text\n",
    "    }\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Save the processed data to a new CSV file\n",
    "# --------------------------\n",
    "final_df = pd.DataFrame(new_rows)\n",
    "output_file = \"pwc_final_concatenated_insights_gzip.parquet\"\n",
    "final_df.to_parquet(output_file, compression='gzip')\n",
    "print(f\"Final Parquet file saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Link</th>\n",
       "      <th>pdf_content</th>\n",
       "      <th>concatenated_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>normalized_concatenated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/03/25</td>\n",
       "      <td>Quality measures and standards for transitioni...</td>\n",
       "      <td>A roadmap to facilitate the transition to VBHC.</td>\n",
       "      <td>https://www.pwc.in/ghost-templates/quality-mea...</td>\n",
       "      <td>Quality measures and standards \\nMarch 2025\\nf...</td>\n",
       "      <td>Quality measures and standards for transitioni...</td>\n",
       "      <td>quality_measures measures_and and_standards st...</td>\n",
       "      <td>quality measure standard transition valuebased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05/03/25</td>\n",
       "      <td>The mutual funds route to Viksit Bharat @2047</td>\n",
       "      <td>A comprehensive roadmap for the evolution of t...</td>\n",
       "      <td>https://www.pwc.in/ghost-templates/the-mutual-...</td>\n",
       "      <td>The mutual funds route \\nto Viksit Bharat @204...</td>\n",
       "      <td>The mutual funds route to Viksit Bharat @2047 ...</td>\n",
       "      <td>the_mutual mutual_funds funds_route route_to t...</td>\n",
       "      <td>mutual fund route viksit bharat 2047 comprehen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04/03/25</td>\n",
       "      <td>Financial health: Transcending from access to ...</td>\n",
       "      <td>Explore India’s financial inclusion journey an...</td>\n",
       "      <td>https://www.pwc.in/ghost-templates/financial-h...</td>\n",
       "      <td>TM\\nMarch 2025\\nFinancial health: \\nTranscendi...</td>\n",
       "      <td>Financial health: Transcending from access to ...</td>\n",
       "      <td>financial_health health_transcending transcend...</td>\n",
       "      <td>financial health transcend access impact explo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/03/25</td>\n",
       "      <td>Towards a climate-resilient future: Strategies...</td>\n",
       "      <td>Explore the comprehensive climate-resilient ac...</td>\n",
       "      <td>https://www.pwc.in/ghost-templates/towards-a-c...</td>\n",
       "      <td>\\nTowards a climate-resilient \\nfuture: Strat...</td>\n",
       "      <td>Towards a climate-resilient future: Strategies...</td>\n",
       "      <td>towards_a a_climateresilient climateresilient_...</td>\n",
       "      <td>towards climateresilient future strategy andam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27/02/25</td>\n",
       "      <td>The retail reinvention paradigm</td>\n",
       "      <td>How brands could up their game</td>\n",
       "      <td>https://www.pwc.in/ghost-templates/retail-rein...</td>\n",
       "      <td>The retail reinvention \\nparadigm\\nHow brands ...</td>\n",
       "      <td>The retail reinvention paradigm How brands cou...</td>\n",
       "      <td>the_retail retail_reinvention reinvention_para...</td>\n",
       "      <td>retail reinvention paradigm brand could game r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date                                              Title  \\\n",
       "0  07/03/25  Quality measures and standards for transitioni...   \n",
       "1  05/03/25      The mutual funds route to Viksit Bharat @2047   \n",
       "2  04/03/25  Financial health: Transcending from access to ...   \n",
       "3  04/03/25  Towards a climate-resilient future: Strategies...   \n",
       "4  27/02/25                    The retail reinvention paradigm   \n",
       "\n",
       "                                         Description  \\\n",
       "0    A roadmap to facilitate the transition to VBHC.   \n",
       "1  A comprehensive roadmap for the evolution of t...   \n",
       "2  Explore India’s financial inclusion journey an...   \n",
       "3  Explore the comprehensive climate-resilient ac...   \n",
       "4                     How brands could up their game   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.pwc.in/ghost-templates/quality-mea...   \n",
       "1  https://www.pwc.in/ghost-templates/the-mutual-...   \n",
       "2  https://www.pwc.in/ghost-templates/financial-h...   \n",
       "3  https://www.pwc.in/ghost-templates/towards-a-c...   \n",
       "4  https://www.pwc.in/ghost-templates/retail-rein...   \n",
       "\n",
       "                                         pdf_content  \\\n",
       "0  Quality measures and standards \\nMarch 2025\\nf...   \n",
       "1  The mutual funds route \\nto Viksit Bharat @204...   \n",
       "2  TM\\nMarch 2025\\nFinancial health: \\nTranscendi...   \n",
       "3   \\nTowards a climate-resilient \\nfuture: Strat...   \n",
       "4  The retail reinvention \\nparadigm\\nHow brands ...   \n",
       "\n",
       "                                   concatenated_text  \\\n",
       "0  Quality measures and standards for transitioni...   \n",
       "1  The mutual funds route to Viksit Bharat @2047 ...   \n",
       "2  Financial health: Transcending from access to ...   \n",
       "3  Towards a climate-resilient future: Strategies...   \n",
       "4  The retail reinvention paradigm How brands cou...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  quality_measures measures_and and_standards st...   \n",
       "1  the_mutual mutual_funds funds_route route_to t...   \n",
       "2  financial_health health_transcending transcend...   \n",
       "3  towards_a a_climateresilient climateresilient_...   \n",
       "4  the_retail retail_reinvention reinvention_para...   \n",
       "\n",
       "                        normalized_concatenated_text  \n",
       "0  quality measure standard transition valuebased...  \n",
       "1  mutual fund route viksit bharat 2047 comprehen...  \n",
       "2  financial health transcend access impact explo...  \n",
       "3  towards climateresilient future strategy andam...  \n",
       "4  retail reinvention paradigm brand could game r...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
