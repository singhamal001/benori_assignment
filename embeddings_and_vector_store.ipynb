{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17 documents from 2 files.\n",
      "Collection 'pwc_kpmg_insights_collection' not found, creating a new one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 17/17 [00:57<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 118 embeddings in Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your OpenAI API key (keep your key secure)\n",
    "# Set the persistence directory via an environment variable.\n",
    "os.environ[\"CHROMA_DB_PERSIST_DIRECTORY\"] = \".chromadb/\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# --- Define Paths for Two Parquet Files ---\n",
    "file_paths = [\n",
    "    \"kpmg_india/kpmg_final_concatenated_insights_gzip.parquet\",  # update with the actual path for the first file\n",
    "    \"pwc_india/pwc_final_concatenated_insights_gzip.parquet\"  # update with the actual path for the second file\n",
    "]\n",
    "\n",
    "# --- Task 1: Load and Concatenate the Parquet Files ---\n",
    "dfs = [pd.read_parquet(fp) for fp in file_paths]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(df)} documents from {len(file_paths)} files.\")\n",
    "\n",
    "# --- Task 2: Define the Text Chunking Function ---\n",
    "def chunk_text(text, chunk_size=8000, overlap=500):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of chunk_size characters,\n",
    "    with each chunk overlapping the previous one by 'overlap' characters.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        if end >= len(text):\n",
    "            break\n",
    "        start = end - overlap  # move back by overlap for next chunk\n",
    "    return chunks\n",
    "\n",
    "# --- Task 3: Initialize Chroma DB Client ---\n",
    "# # Instantiate the Chroma client without extra keyword arguments.\n",
    "client = chromadb.Client()\n",
    "collection_name = \"pwc_kpmg_insights_collection\"\n",
    "\n",
    "# Try to retrieve the collection. If it doesn't exist, create a new one.\n",
    "try:\n",
    "    collection = client.get_collection(collection_name)\n",
    "except Exception as e:\n",
    "    print(f\"Collection '{collection_name}' not found, creating a new one.\")\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "# --- Task 4: Process Each Document to Create and Store Embeddings ---\n",
    "embedding_ids = []\n",
    "embedding_vectors = []\n",
    "metadatas = []\n",
    "documents = []\n",
    "embedding_count = 0  # ordinal counter for embeddings\n",
    "\n",
    "# Iterate over each row (document) in the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
    "    # Get the text from \"normalized_concatenated_text\"\n",
    "    text = row[\"normalized_concatenated_text\"]\n",
    "    \n",
    "    # Retrieve metadata fields. If there is no \"source\" column, fallback to \"Link\".\n",
    "    source = row.get(\"source\", row.get(\"link\", \"\"))\n",
    "    title = row[\"title\"]\n",
    "    date = row[\"date\"]\n",
    "    \n",
    "    # Chunk the text into segments of 8000 characters with 500 overlap\n",
    "    chunks = chunk_text(text, chunk_size=8000, overlap=500)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding_count += 1\n",
    "        # Generate embedding using OpenAI's API\n",
    "        response = openai.Embedding.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=chunk\n",
    "        )\n",
    "        embedding_vector = response[\"data\"][0][\"embedding\"]\n",
    "        \n",
    "        # Create a unique embedding ID combining the document index and chunk index\n",
    "        embedding_id = f\"{index}_{i}\"\n",
    "        \n",
    "        # Build metadata for this embedding\n",
    "        metadata = {\n",
    "            \"source\": source,\n",
    "            \"title\": title,\n",
    "            \"date\": date,\n",
    "            \"embedding_number\": embedding_count\n",
    "        }\n",
    "        \n",
    "        # Append results for later bulk insertion\n",
    "        embedding_ids.append(embedding_id)\n",
    "        embedding_vectors.append(embedding_vector)\n",
    "        metadatas.append(metadata)\n",
    "        documents.append(chunk)\n",
    "\n",
    "# --- Task 5: Store the Embeddings in Chroma DB ---\n",
    "collection.add(\n",
    "    ids=embedding_ids,\n",
    "    embeddings=embedding_vectors,\n",
    "    metadatas=metadatas,\n",
    "    documents=documents\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(embedding_ids)} embeddings in Chroma DB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete to exported_collection.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Get all items from the collection\n",
    "results = collection.get(\n",
    "    include=[\"embeddings\", \"metadatas\", \"documents\"]\n",
    ")\n",
    "\n",
    "def convert_ndarray(obj):\n",
    "    \"\"\"Recursively convert NumPy ndarrays to lists.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_ndarray(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_ndarray(val) for key, val in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert the results to be JSON serializable.\n",
    "results_serializable = convert_ndarray(results)\n",
    "\n",
    "# Now, export to JSON\n",
    "with open(\"exported_collection.json\", \"w\") as f:\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"Export complete to exported_collection.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
